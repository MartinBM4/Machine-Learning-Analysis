{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# Lab assignment: classifying digits with Deep Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "<img src=\"img/mnist.jpeg\" style=\"width:480px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "In this assignment we will face the problem of recognizing handwritten digits. We will see how in order to achieve maximum effectiveness we will need to resort to several Deep Learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "## Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "Throughout this notebook you will find empty cells that you will need to fill with your own code. Follow the instructions in the notebook and pay special attention to the following symbols.\n",
    "\n",
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>You will need to solve a question by writing your own code or answer in the cell immediately below, or in a different file as instructed.</td></tr>\n",
    " <tr><td><img src=\"img/exclamation.png\" style=\"width:80px;height:80px;\"></td><td>This is a hint or useful observation that can help you solve this assignment. You are not expected to write any solution, but you should pay attention to them to understand the assignment.</td></tr>\n",
    " <tr><td><img src=\"img/pro.png\" style=\"width:80px;height:80px;\"></td><td>This is an advanced and voluntary exercise that can help you gain a deeper knowledge into the topic. Good luck!</td></tr>\n",
    "</table>\n",
    "\n",
    "During the assigment you will make use of several Python packages that might not be installed in your machine. If that is the case, you can install new Python packages with\n",
    "\n",
    "    conda install PACKAGENAME\n",
    "    \n",
    "if you are using Python Anaconda. Else you should use\n",
    "\n",
    "    pip install PACKAGENAME\n",
    "\n",
    "You will need the following packages for this particular assignment. Make sure they are available before proceeding:\n",
    "\n",
    "* **numpy**\n",
    "* **keras**\n",
    "* **matplotlib**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will embed any plots into the notebook instead of generating a new window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, if you need any help on the usage of a Python function you can place the writing cursor over its name and press Caps+Shift to produce a pop-out with related documentation. This will only work inside code cells. \n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Keras library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will make use of the <a href=http://keras.io/>keras</a> Deep Learning library for Python. This library allows building several kinds of shallow and deep networks, following either a sequential or a graph architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The handwritten digits recognition problem we will face is already included as a testbed in keras. Loading it only requires invoking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded **X** variables are made up of the handwritten digits to classify, while the **y** variables contain the labels of the corresponding X images, telling the digits such images represent. We will use the **train** data to build our neural network, while we will use the **test** data to measure the performance of such network on an independent dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how many images we have for training and testing as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we can take a look at the shape, width and height in pixels, of an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the images we are working with by means of using the matplotlib library. Here we are taking the first training image and painting it with a grayscale colormap. Also we are printing the corresponding class value, to ensure the labeling of the digit is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit class: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], 'gray')\n",
    "print(\"Digit class:\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Use the cell below to plot some other image in the training dataset, along with its corresponding digit class number. Can you find any hard to identify digit?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit class: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[500], 'gray')\n",
    "print(\"Digit class:\", y_train[500])\n",
    "\n",
    "#I think this number is not completed. Confuse between 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting building networks we should always normalize our data. This usually means substracting the mean from each variable and then dividing by the standard deviation. However in grayscale images like the ones we are working with all variables represent pixel intensities, and are bound to integers in the range [0, 255]. We can thus perform a simple initialization by just compressing this range to [0, 1]. We should also transform the data to real numbers (float) while performing this operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Perform the same normalization for the test data\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the outputs, normalization as such is not required, but we should change the class encoding to something more akin to neural networks. Instead of having a single integer ranging [0,9] to encode the different classes, we will use a <a href=https://en.wikipedia.org/wiki/One-hot>one-hot vector encoding</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "Y_train = np_utils.to_categorical(y_train, 10) # We have 10 classes to codify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the transformation was correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train[0]) #this is the number 5 (0,1,2,3,4,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Repeat the same encoding for the classes of the test data\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start trying to solve the problem with the simplest neural network: a Perceptron. This means a neural network with no hidden layers, just some weights going from input to output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a network in Keras begins by choosing the type of architecture. We can either decide to build a **Sequential** network, where each layer is followed by another one in a chain, or a **Graph** network, where divergences and loops of layers can take place. In this practice we will restrict ourselves to the Sequential architecture. We can initialize a Sequential network with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the network has been initialized this way, we just need to iteratively add the desired layers. For the perceptron network we only require a \"classic\" layer of weights from input to output. Such layer is name **Dense** in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually creating a dense layer only involves specifying the number of outputs units of such layer. But since this will be the first layer in the network we also need to specify the number of inputs. Our inputs are images of 28x28 pixels, which makes 784 input values. As for the outputs, we have 10 classes in our problem, so that makes 10 output units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "denselayer = Dense(10, input_shape=(784,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add the layer to network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(denselayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we have declared the layer of weights from inputs to outputs. Since we are facing a classification problem we should also add an activation function to the output units that enforces the output values to the range [0,1]. We will choose a softmax activation for doing so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Activation\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this the definition of our network is completed. We can get a text description of the network by calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the network architecture the next step involves compiling the network. Compilation is an automatic process that transforms the network definition into an equivalent symbolic formulation for which derivatives can be computed, thus allowing learning through backpropagation. The only input required in this process is choosing the loss function the network should minimize, and the optimizer used for learning.\n",
    "\n",
    "For our current network we will use **categorical crossentropy** as the loss function, as it is suitable for multiclass classification problems. As for the optimizer, we will use **Stochastic Gradient Descent**. We will also include the **classification accuracy** as a metric to measure the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now almost ready to adjust the network parameters through training over our data. There is only one small detail left: our data is in the form of bidimensional images, while a perceptron only understands training patterns as one-dimensional vectors of data. We should then transform the data to vector form to input it into the network, something we can do with the **reshape** method of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvectors = X_train.reshape(60000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check now that our training data has become a matrix of 60000 training patterns (rows) and 784 variables (pixels) per pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainvectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Perform a similar transformation for the X_test data, saving the reshaped data into a variable named *testvectors*. Note that the number of pattens in the test data is different from the number of patterns in the training data.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testvectors = X_test.reshape(10000, 784)\n",
    "testvectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can invoke the **fit** method of the network, which will perform the training process. It is done as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/master/Aplicaciones/anaconda-navigator/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 2s - loss: 1.2739 - accuracy: 0.7008\n",
      "Epoch 2/20\n",
      " - 2s - loss: 0.7174 - accuracy: 0.8371\n",
      "Epoch 3/20\n",
      " - 2s - loss: 0.5884 - accuracy: 0.8585\n",
      "Epoch 4/20\n",
      " - 2s - loss: 0.5266 - accuracy: 0.8683\n",
      "Epoch 5/20\n",
      " - 2s - loss: 0.4888 - accuracy: 0.8755\n",
      "Epoch 6/20\n",
      " - 2s - loss: 0.4628 - accuracy: 0.8804\n",
      "Epoch 7/20\n",
      " - 4s - loss: 0.4436 - accuracy: 0.8835\n",
      "Epoch 8/20\n",
      " - 4s - loss: 0.4286 - accuracy: 0.8864\n",
      "Epoch 9/20\n",
      " - 2s - loss: 0.4165 - accuracy: 0.8891\n",
      "Epoch 10/20\n",
      " - 1s - loss: 0.4065 - accuracy: 0.8910\n",
      "Epoch 11/20\n",
      " - 2s - loss: 0.3980 - accuracy: 0.8927\n",
      "Epoch 12/20\n",
      " - 4s - loss: 0.3907 - accuracy: 0.8942\n",
      "Epoch 13/20\n",
      " - 4s - loss: 0.3843 - accuracy: 0.8957\n",
      "Epoch 14/20\n",
      " - 2s - loss: 0.3786 - accuracy: 0.8972\n",
      "Epoch 15/20\n",
      " - 3s - loss: 0.3736 - accuracy: 0.8979\n",
      "Epoch 16/20\n",
      " - 3s - loss: 0.3690 - accuracy: 0.8990\n",
      "Epoch 17/20\n",
      " - 2s - loss: 0.3649 - accuracy: 0.8998\n",
      "Epoch 18/20\n",
      " - 2s - loss: 0.3612 - accuracy: 0.9007\n",
      "Epoch 19/20\n",
      " - 3s - loss: 0.3577 - accuracy: 0.9010\n",
      "Epoch 20/20\n",
      " - 3s - loss: 0.3545 - accuracy: 0.9021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7ff85d372c10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    trainvectors, # Training data\n",
    "    Y_train, # Labels of training data\n",
    "    batch_size=128, # Batch size for the optimizer algorithm\n",
    "    nb_epoch=20, # Number of epochs to run the optimizer algorithm\n",
    "    verbose=2 # Level of verbosity of the log messages\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our neural network model is trained, we can obtain class predictions for the test set as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_classes(testvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for instance, the first image in the test set and its predicted class are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real class 7 predicted class 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[0], 'gray')\n",
    "print(\"Real class\", y_test[0], \"predicted class\", preds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Compare the predicted and real classes for other images in the test set. Can you find any error?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real class 5 predicted class 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[8], 'gray')\n",
    "print(\"Real class\", y_test[8], \"predicted class\", preds[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ooooops!  :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/exclamation.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "You can spot all the errors in an automated way by comparing *y_test* against *preds* and getting the indexes of the mismatching elements. The function <a href=http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.where.html>np.where</a> might also help.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overall accuracy measure can also be obtained by means of the **evaluate** method of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 37us/step\n",
      "Test loss 0.3358705408811569\n",
      "Test accuracy 0.909500002861023\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testvectors, Y_test)\n",
    "print(\"Test loss\", score[0])\n",
    "print(\"Test accuracy\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Do you think the level of accuracy obtained is good enough for a real application? Suppose that every time a single digit is misclasified a package might be sent to the wrong address, and ZIP codes in the USA are made of 9 digits. What is the probability of sending a package to a wrong address?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong address probability:  38.742048900000015 %\n"
     ]
    }
   ],
   "source": [
    "prob = 9*0.1*0.9*0.9*0.9*0.9*0.9*0.9*0.9*0.9\n",
    "print(\"Wrong address probability: \",prob*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it is a fairly high probability. The machine would fail approximately one out of every 3 shipments.\n",
    "\n",
    "We need a better accuracy changing the struct of the network or increasing the epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A potential way to attain further improvements might be to create a deeper network, by adding layers of hidden units. This is easy to do in Keras, just by defining a new architecture with several Dense layers. For example, to build a network with a hidden layer of 10 units with sigmoid activation we would write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, input_shape=(784,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,960\n",
      "Trainable params: 7,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Compile the defined network and train it with the data. Then measure the accuracy over the test data. Have you managed to get any improvement over the previous Perceptron model?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/master/Aplicaciones/anaconda-navigator/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 1s - loss: 2.2398 - accuracy: 0.1793\n",
      "Epoch 2/20\n",
      " - 2s - loss: 2.0097 - accuracy: 0.5265\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.8252 - accuracy: 0.6399\n",
      "Epoch 4/20\n",
      " - 1s - loss: 1.6626 - accuracy: 0.6849\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.5176 - accuracy: 0.7175\n",
      "Epoch 6/20\n",
      " - 2s - loss: 1.3889 - accuracy: 0.7442\n",
      "Epoch 7/20\n",
      " - 1s - loss: 1.2756 - accuracy: 0.7682\n",
      "Epoch 8/20\n",
      " - 1s - loss: 1.1765 - accuracy: 0.7879\n",
      "Epoch 9/20\n",
      " - 1s - loss: 1.0899 - accuracy: 0.8007\n",
      "Epoch 10/20\n",
      " - 1s - loss: 1.0144 - accuracy: 0.8124\n",
      "Epoch 11/20\n",
      " - 1s - loss: 0.9485 - accuracy: 0.8219\n",
      "Epoch 12/20\n",
      " - 1s - loss: 0.8911 - accuracy: 0.8292\n",
      "Epoch 13/20\n",
      " - 1s - loss: 0.8407 - accuracy: 0.8358\n",
      "Epoch 14/20\n",
      " - 1s - loss: 0.7964 - accuracy: 0.8418\n",
      "Epoch 15/20\n",
      " - 1s - loss: 0.7573 - accuracy: 0.8474\n",
      "Epoch 16/20\n",
      " - 1s - loss: 0.7227 - accuracy: 0.8515\n",
      "Epoch 17/20\n",
      " - 1s - loss: 0.6919 - accuracy: 0.8558\n",
      "Epoch 18/20\n",
      " - 1s - loss: 0.6644 - accuracy: 0.8598\n",
      "Epoch 19/20\n",
      " - 1s - loss: 0.6399 - accuracy: 0.8628\n",
      "Epoch 20/20\n",
      " - 1s - loss: 0.6179 - accuracy: 0.8657\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    trainvectors, # Training data\n",
    "    Y_train, # Labels of training data\n",
    "    batch_size=128, # Batch size for the optimizer algorithm\n",
    "    nb_epoch=20, # Number of epochs to run the optimizer algorithm\n",
    "    verbose=2 # Level of verbosity of the log messages\n",
    ")\n",
    "\n",
    "preds = model.predict_classes(testvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A somewhat worse result has been obtained than the previous one, that may be why the data is randomly chosen in learning.\n",
    "If we train again and model, we can see that we have a different precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real class 5 predicted class 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[8], 'gray')\n",
    "print(\"Real class\", y_test[8], \"predicted class\", preds[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fail again :-("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 42us/step\n",
      "Test loss 0.5911996202468872\n",
      "Test accuracy 0.8736000061035156\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testvectors, Y_test)\n",
    "print(\"Test loss\", score[0])\n",
    "print(\"Test accuracy\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong address probability:  38.972229176450604 %\n"
     ]
    }
   ],
   "source": [
    "prob = 9*(1-0.89)*0.89*0.89*0.89*0.89*0.89*0.89*0.89*0.89\n",
    "print(\"Wrong address probability: \",prob*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability is a bit worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning the network design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the performance of the multilayer perceptron we will use the following:\n",
    "* Increase the number of hidden units\n",
    "* Use a better activation function: rectified linear\n",
    "* Use a better optimizer: adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This boils down to defining the network as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Compile the defined network, choosing \"adam\" as the optimization algorithm, and train it with the data. Then measure the accuracy over the test data. Did these changes give rise to better results?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/master/Aplicaciones/anaconda-navigator/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 2s - loss: 0.3732 - accuracy: 0.8985\n",
      "Epoch 2/20\n",
      " - 2s - loss: 0.1705 - accuracy: 0.9517\n",
      "Epoch 3/20\n",
      " - 2s - loss: 0.1235 - accuracy: 0.9637\n",
      "Epoch 4/20\n",
      " - 3s - loss: 0.0965 - accuracy: 0.9726\n",
      "Epoch 5/20\n",
      " - 3s - loss: 0.0801 - accuracy: 0.9773\n",
      "Epoch 6/20\n",
      " - 2s - loss: 0.0679 - accuracy: 0.9801\n",
      "Epoch 7/20\n",
      " - 2s - loss: 0.0584 - accuracy: 0.9830\n",
      "Epoch 8/20\n",
      " - 2s - loss: 0.0504 - accuracy: 0.9854\n",
      "Epoch 9/20\n",
      " - 2s - loss: 0.0431 - accuracy: 0.9876\n",
      "Epoch 10/20\n",
      " - 2s - loss: 0.0379 - accuracy: 0.9893\n",
      "Epoch 11/20\n",
      " - 2s - loss: 0.0334 - accuracy: 0.9905\n",
      "Epoch 12/20\n",
      " - 2s - loss: 0.0287 - accuracy: 0.9919\n",
      "Epoch 13/20\n",
      " - 2s - loss: 0.0257 - accuracy: 0.9930\n",
      "Epoch 14/20\n",
      " - 2s - loss: 0.0219 - accuracy: 0.9942\n",
      "Epoch 15/20\n",
      " - 3s - loss: 0.0188 - accuracy: 0.9955\n",
      "Epoch 16/20\n",
      " - 2s - loss: 0.0159 - accuracy: 0.9967\n",
      "Epoch 17/20\n",
      " - 2s - loss: 0.0146 - accuracy: 0.9966\n",
      "Epoch 18/20\n",
      " - 2s - loss: 0.0134 - accuracy: 0.9970\n",
      "Epoch 19/20\n",
      " - 2s - loss: 0.0112 - accuracy: 0.9979\n",
      "Epoch 20/20\n",
      " - 2s - loss: 0.0098 - accuracy: 0.9980\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    trainvectors, # Training data\n",
    "    Y_train, # Labels of training data\n",
    "    batch_size=128, # Batch size for the optimizer algorithm\n",
    "    nb_epoch=20, # Number of epochs to run the optimizer algorithm\n",
    "    verbose=2 # Level of verbosity of the log messages\n",
    ")\n",
    "\n",
    "preds = model.predict_classes(testvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real class 5 predicted class 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[8], 'gray')\n",
    "print(\"Real class\", y_test[8], \"predicted class\", preds[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bingo!! :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 109us/step\n",
      "Test loss 0.07648511282019609\n",
      "Test accuracy 0.9783999919891357\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testvectors, Y_test)\n",
    "print(\"Test loss\", score[0])\n",
    "print(\"Test accuracy\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Define a new network with two hidden layers, each of 512 hidden units with rectified linear activation. For the output use the softmax activation. Compile the defined network, choosing \"adam\" as the optimization algorithm, and train it with the data. Then measure the accuracy over the test data. How are you doing now?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/master/Aplicaciones/anaconda-navigator/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 12s - loss: 0.2163 - accuracy: 0.9360\n",
      "Epoch 2/20\n",
      " - 13s - loss: 0.0778 - accuracy: 0.9762\n",
      "Epoch 3/20\n",
      " - 11s - loss: 0.0516 - accuracy: 0.9842\n",
      "Epoch 4/20\n",
      " - 10s - loss: 0.0364 - accuracy: 0.9883\n",
      "Epoch 5/20\n",
      " - 10s - loss: 0.0272 - accuracy: 0.9912\n",
      "Epoch 6/20\n",
      " - 10s - loss: 0.0222 - accuracy: 0.9927\n",
      "Epoch 7/20\n",
      " - 8s - loss: 0.0174 - accuracy: 0.9939\n",
      "Epoch 8/20\n",
      " - 11s - loss: 0.0155 - accuracy: 0.9948\n",
      "Epoch 9/20\n",
      " - 11s - loss: 0.0154 - accuracy: 0.9951\n",
      "Epoch 10/20\n",
      " - 13s - loss: 0.0091 - accuracy: 0.9970\n",
      "Epoch 11/20\n",
      " - 13s - loss: 0.0150 - accuracy: 0.9948\n",
      "Epoch 12/20\n",
      " - 13s - loss: 0.0094 - accuracy: 0.9970\n",
      "Epoch 13/20\n",
      " - 13s - loss: 0.0109 - accuracy: 0.9962\n",
      "Epoch 14/20\n",
      " - 9s - loss: 0.0078 - accuracy: 0.9974\n",
      "Epoch 15/20\n",
      " - 11s - loss: 0.0120 - accuracy: 0.9960\n",
      "Epoch 16/20\n",
      " - 11s - loss: 0.0102 - accuracy: 0.9964\n",
      "Epoch 17/20\n",
      " - 9s - loss: 0.0071 - accuracy: 0.9977\n",
      "Epoch 18/20\n",
      " - 9s - loss: 0.0061 - accuracy: 0.9980\n",
      "Epoch 19/20\n",
      " - 12s - loss: 0.0078 - accuracy: 0.9978\n",
      "Epoch 20/20\n",
      " - 9s - loss: 0.0051 - accuracy: 0.9985\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    trainvectors, # Training data\n",
    "    Y_train, # Labels of training data\n",
    "    batch_size=128, # Batch size for the optimizer algorithm\n",
    "    nb_epoch=20, # Number of epochs to run the optimizer algorithm\n",
    "    verbose=2 # Level of verbosity of the log messages\n",
    ")\n",
    "\n",
    "preds = model.predict_classes(testvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real class 5 predicted class 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[8], 'gray')\n",
    "print(\"Real class\", y_test[8], \"predicted class\", preds[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 124us/step\n",
      "Test loss 0.09381880935045912\n",
      "Test accuracy 0.9840999841690063\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testvectors, Y_test)\n",
    "print(\"Test loss\", score[0])\n",
    "print(\"Test accuracy\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization can help improve the performance of a network, specially when the number of network parameters becomes large and this leads to better performance in training data than in test data, which is to say, overfitting. One of the most simple and effective ways of doing so is by using **dropout**. In Keras dropout is imposed on a layer by adding a **Dropout** layer just after the activation layer in which we wish to impose regularization. For instance, to create a network with a hidden layer with a dropout of a 30% probability of dropping an input unit we would write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would just need to add this layer to the model to produce the dropout effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Define a new network with two hidden layers, each of 512 hidden units with rectified linear activation. Both hidden layers should have a Dropout of 40%. For the output use the softmax activation. Compile the defined network, choosing \"adam\" as the optimization algorithm, and train it with the data. Then measure the accuracy over the test data. Has dropout helped?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/master/Aplicaciones/anaconda-navigator/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 11s - loss: 0.3005 - accuracy: 0.9084\n",
      "Epoch 2/20\n",
      " - 12s - loss: 0.1337 - accuracy: 0.9587\n",
      "Epoch 3/20\n",
      " - 14s - loss: 0.1016 - accuracy: 0.9685\n",
      "Epoch 4/20\n",
      " - 11s - loss: 0.0855 - accuracy: 0.9730\n",
      "Epoch 5/20\n",
      " - 11s - loss: 0.0742 - accuracy: 0.9765\n",
      "Epoch 6/20\n",
      " - 13s - loss: 0.0662 - accuracy: 0.9787\n",
      "Epoch 7/20\n",
      " - 10s - loss: 0.0618 - accuracy: 0.9798\n",
      "Epoch 8/20\n",
      " - 10s - loss: 0.0540 - accuracy: 0.9830\n",
      "Epoch 9/20\n",
      " - 11s - loss: 0.0511 - accuracy: 0.9835\n",
      "Epoch 10/20\n",
      " - 10s - loss: 0.0479 - accuracy: 0.9840\n",
      "Epoch 11/20\n",
      " - 10s - loss: 0.0446 - accuracy: 0.9855\n",
      "Epoch 12/20\n",
      " - 9s - loss: 0.0413 - accuracy: 0.9865\n",
      "Epoch 13/20\n",
      " - 12s - loss: 0.0394 - accuracy: 0.9871\n",
      "Epoch 14/20\n",
      " - 12s - loss: 0.0391 - accuracy: 0.9873\n",
      "Epoch 15/20\n",
      " - 11s - loss: 0.0356 - accuracy: 0.9879\n",
      "Epoch 16/20\n",
      " - 10s - loss: 0.0351 - accuracy: 0.9890\n",
      "Epoch 17/20\n",
      " - 12s - loss: 0.0322 - accuracy: 0.9898\n",
      "Epoch 18/20\n",
      " - 10s - loss: 0.0333 - accuracy: 0.9894\n",
      "Epoch 19/20\n",
      " - 11s - loss: 0.0302 - accuracy: 0.9900\n",
      "Epoch 20/20\n",
      " - 11s - loss: 0.0289 - accuracy: 0.9907\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    trainvectors, # Training data\n",
    "    Y_train, # Labels of training data\n",
    "    batch_size=128, # Batch size for the optimizer algorithm\n",
    "    nb_epoch=20, # Number of epochs to run the optimizer algorithm\n",
    "    verbose=2 # Level of verbosity of the log messages\n",
    ")\n",
    "\n",
    "preds = model.predict_classes(testvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real class 5 predicted class 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[8], 'gray')\n",
    "print(\"Real class\", y_test[8], \"predicted class\", preds[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 89us/step\n",
      "Test loss 0.07598867102102613\n",
      "Test accuracy 0.9822999835014343\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testvectors, Y_test)\n",
    "print(\"Test loss\", score[0])\n",
    "print(\"Test accuracy\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Try training a network with more hidden layers. Does the performance improve in any way by doing this?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/master/Aplicaciones/anaconda-navigator/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 12s - loss: 0.5651 - accuracy: 0.8279\n",
      "Epoch 2/20\n",
      " - 11s - loss: 0.2305 - accuracy: 0.9413\n",
      "Epoch 3/20\n",
      " - 12s - loss: 0.1781 - accuracy: 0.9550\n",
      "Epoch 4/20\n",
      " - 10s - loss: 0.1498 - accuracy: 0.9620\n",
      "Epoch 5/20\n",
      " - 11s - loss: 0.1279 - accuracy: 0.9672\n",
      "Epoch 6/20\n",
      " - 9s - loss: 0.1179 - accuracy: 0.9700\n",
      "Epoch 7/20\n",
      " - 11s - loss: 0.1051 - accuracy: 0.9724\n",
      "Epoch 8/20\n",
      " - 9s - loss: 0.0990 - accuracy: 0.9742\n",
      "Epoch 9/20\n",
      " - 10s - loss: 0.0919 - accuracy: 0.9765\n",
      "Epoch 10/20\n",
      " - 10s - loss: 0.0857 - accuracy: 0.9778\n",
      "Epoch 11/20\n",
      " - 11s - loss: 0.0798 - accuracy: 0.9794\n",
      "Epoch 12/20\n",
      " - 12s - loss: 0.0764 - accuracy: 0.9794\n",
      "Epoch 13/20\n",
      " - 13s - loss: 0.0729 - accuracy: 0.9812\n",
      "Epoch 14/20\n",
      " - 10s - loss: 0.0699 - accuracy: 0.9815\n",
      "Epoch 15/20\n",
      " - 11s - loss: 0.0681 - accuracy: 0.9821\n",
      "Epoch 16/20\n",
      " - 12s - loss: 0.0650 - accuracy: 0.9834\n",
      "Epoch 17/20\n",
      " - 11s - loss: 0.0609 - accuracy: 0.9832\n",
      "Epoch 18/20\n",
      " - 12s - loss: 0.0594 - accuracy: 0.9844\n",
      "Epoch 19/20\n",
      " - 12s - loss: 0.0567 - accuracy: 0.9846\n",
      "Epoch 20/20\n",
      " - 11s - loss: 0.0560 - accuracy: 0.9849\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    trainvectors, # Training data\n",
    "    Y_train, # Labels of training data\n",
    "    batch_size=128, # Batch size for the optimizer algorithm\n",
    "    nb_epoch=20, # Number of epochs to run the optimizer algorithm\n",
    "    verbose=2 # Level of verbosity of the log messages\n",
    ")\n",
    "\n",
    "preds = model.predict_classes(testvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real class 5 predicted class 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[8], 'gray')\n",
    "print(\"Real class\", y_test[8], \"predicted class\", preds[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 104us/step\n",
      "Test loss 0.09240808267106432\n",
      "Test accuracy 0.9807999730110168\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testvectors, Y_test)\n",
    "print(\"Test loss\", score[0])\n",
    "print(\"Test accuracy\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further improve on this image recognition problem we need network layers that do consider the data as images, and take into account closeness of pixels to make decisions instead of just throwing all pixel data into a fully connected network and expect intelligence to emerge from chaos. **Convolutional** and **Pooling** layers are the best way to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the data as tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While for the perceptrons above we vectorized the data to fit into the perceptron framework, for convolutional networks we will need to shape the data in the form of a **4-dimensional tensor**. The dimensions of such tensor represent the following:\n",
    "* Image index (e.g. 3th image in the dataset)\n",
    "* Row index\n",
    "* Column index\n",
    "* Channel index (e.g. colour channel in colored images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again make use of the reshape function to perform this transformation. We have 60000 images in our training set, and those images have 28 rows x 28 columns. Since these images are grayscale, the channel dimension only contains one channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintensor = X_train.reshape(60000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Repeat the transformation for the test data. Save the resulting tensor in a variable named *testtensor*.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtensor = X_test.reshape(10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining a convolutional network, Convolution and Pooling layers work together. The most popular way of using these layers is in the following pattern:\n",
    "* A Convolution layer with rectified linear activations\n",
    "* A Pooling layer\n",
    "* Dropout (if regularization wants to be enforced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can thus define a minimal convolutional network as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/master/Aplicaciones/anaconda-navigator/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(28, 28, 1..., padding=\"valid\")`\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "kernel_size = 3 # Size of the kernel for the convolution layers\n",
    "pool_size = 2 # Size of the pooling region for the pooling layers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, # Number convolution channels to generate\n",
    "                        kernel_size, kernel_size, # Size of convolution kernels\n",
    "                        border_mode='valid', # Strategy to deal with borders\n",
    "                        input_shape=(img_rows, img_cols, 1))) # Size = image rows x image columns x channels\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an issue, though: at some point we need to transform the tensor data into a vector, as the output of the network should be a vector of 10 values, representing class probabilities. We can do this by using a **Flatten** layer. Then we can add a standard Dense layer to produce the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Flatten\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Compile the defined network, choosing \"adam\" as the optimization algorithm, and train it with the data. Use the tensor data you prepared above, not the vectorized data. Then measure the accuracy over the test data. Have the Convolution and MaxPooling helped?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/master/Aplicaciones/anaconda-navigator/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 25s - loss: 0.0307 - accuracy: 0.9901\n",
      "Epoch 2/20\n",
      " - 25s - loss: 0.0308 - accuracy: 0.9897\n",
      "Epoch 3/20\n",
      " - 23s - loss: 0.0304 - accuracy: 0.9902\n",
      "Epoch 4/20\n",
      " - 26s - loss: 0.0279 - accuracy: 0.9912\n",
      "Epoch 5/20\n",
      " - 26s - loss: 0.0272 - accuracy: 0.9911\n",
      "Epoch 6/20\n",
      " - 24s - loss: 0.0254 - accuracy: 0.9919\n",
      "Epoch 7/20\n",
      " - 26s - loss: 0.0251 - accuracy: 0.9921\n",
      "Epoch 8/20\n",
      " - 19s - loss: 0.0249 - accuracy: 0.9919\n",
      "Epoch 9/20\n",
      " - 23s - loss: 0.0250 - accuracy: 0.9918\n",
      "Epoch 10/20\n",
      " - 25s - loss: 0.0243 - accuracy: 0.9918\n",
      "Epoch 11/20\n",
      " - 24s - loss: 0.0220 - accuracy: 0.9929\n",
      "Epoch 12/20\n",
      " - 18s - loss: 0.0218 - accuracy: 0.9929\n",
      "Epoch 13/20\n",
      " - 24s - loss: 0.0214 - accuracy: 0.9932\n",
      "Epoch 14/20\n",
      " - 27s - loss: 0.0210 - accuracy: 0.9929\n",
      "Epoch 15/20\n",
      " - 24s - loss: 0.0206 - accuracy: 0.9933\n",
      "Epoch 16/20\n",
      " - 20s - loss: 0.0198 - accuracy: 0.9937\n",
      "Epoch 17/20\n",
      " - 25s - loss: 0.0188 - accuracy: 0.9939\n",
      "Epoch 18/20\n",
      " - 28s - loss: 0.0199 - accuracy: 0.9936\n",
      "Epoch 19/20\n",
      " - 25s - loss: 0.0187 - accuracy: 0.9940\n",
      "Epoch 20/20\n",
      " - 26s - loss: 0.0171 - accuracy: 0.9941\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    traintensor, # Training data\n",
    "    Y_train, # Labels of training data\n",
    "    batch_size=128, # Batch size for the optimizer algorithm\n",
    "    nb_epoch=20, # Number of epochs to run the optimizer algorithm\n",
    "    verbose=2 # Level of verbosity of the log messages\n",
    ")\n",
    "\n",
    "preds = model.predict_classes(testtensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 110us/step\n",
      "Test loss 0.05194392412010929\n",
      "Test accuracy 0.9850000143051147\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testtensor, Y_test)\n",
    "print(\"Test loss\", score[0])\n",
    "print(\"Test accuracy\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Build and train a convolutional network with the following layers:\n",
    "<ul>\n",
    "     <li>A Convolution layer of 32 channels, kernel size 3 and rectified linear activation</li>\n",
    "     <li>Another Convolution layer of 32 channels, kernel size 3 and rectified linear activation</li>\n",
    "     <li>A MaxPooling layer of size 2</li>\n",
    "     <li>A 25% Dropout</li>\n",
    "     <li>A Flatten layer</li>\n",
    "     <li>A Dense layer with 128 units and rectified linear activation</li>\n",
    "     <li>A 50% Dropout</li>\n",
    "     <li>An output Dense layer with softmax activation</li>\n",
    "</ul>\n",
    "Has the added complexity improved the accuracy results?    \n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/master/Aplicaciones/anaconda-navigator/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(28, 28, 1..., padding=\"valid\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/master/Aplicaciones/anaconda-navigator/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(28, 28, 1..., padding=\"valid\")`\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "kernel_size = 3 # Size of the kernel for the convolution layers\n",
    "pool_size = 2 # Size of the pooling region for the pooling layers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, # Number convolution channels to generate\n",
    "                        kernel_size, kernel_size, # Size of convolution kernels\n",
    "                        border_mode='valid', # Strategy to deal with borders\n",
    "                        input_shape=(img_rows, img_cols, 1))) # Size = image rows x image columns x channels\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(32, # Number convolution channels to generate\n",
    "                        kernel_size, kernel_size, # Size of convolution kernels\n",
    "                        border_mode='valid', # Strategy to deal with borders\n",
    "                        input_shape=(img_rows, img_cols, 1))) # Size = image rows x image columns x channels\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/master/Aplicaciones/anaconda-navigator/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 37s - loss: 0.2163 - accuracy: 0.9343\n",
      "Epoch 2/20\n",
      " - 42s - loss: 0.0668 - accuracy: 0.9790\n",
      "Epoch 3/20\n",
      " - 26s - loss: 0.0486 - accuracy: 0.9845\n",
      "Epoch 4/20\n",
      " - 40s - loss: 0.0365 - accuracy: 0.9883\n",
      "Epoch 5/20\n",
      " - 45s - loss: 0.0320 - accuracy: 0.9900\n",
      "Epoch 6/20\n",
      " - 46s - loss: 0.0265 - accuracy: 0.9914\n",
      "Epoch 7/20\n",
      " - 46s - loss: 0.0237 - accuracy: 0.9922\n",
      "Epoch 8/20\n",
      " - 44s - loss: 0.0202 - accuracy: 0.9934\n",
      "Epoch 9/20\n",
      " - 44s - loss: 0.0183 - accuracy: 0.9938\n",
      "Epoch 10/20\n",
      " - 44s - loss: 0.0155 - accuracy: 0.9945\n",
      "Epoch 11/20\n",
      " - 42s - loss: 0.0159 - accuracy: 0.9947\n",
      "Epoch 12/20\n",
      " - 55s - loss: 0.0136 - accuracy: 0.9953\n",
      "Epoch 13/20\n",
      " - 48s - loss: 0.0135 - accuracy: 0.9955\n",
      "Epoch 14/20\n",
      " - 43s - loss: 0.0112 - accuracy: 0.9963\n",
      "Epoch 15/20\n",
      " - 43s - loss: 0.0102 - accuracy: 0.9967\n",
      "Epoch 16/20\n",
      " - 42s - loss: 0.0100 - accuracy: 0.9965\n",
      "Epoch 17/20\n",
      " - 43s - loss: 0.0098 - accuracy: 0.9967\n",
      "Epoch 18/20\n",
      " - 43s - loss: 0.0095 - accuracy: 0.9970\n",
      "Epoch 19/20\n",
      " - 29s - loss: 0.0092 - accuracy: 0.9968\n",
      "Epoch 20/20\n",
      " - 43s - loss: 0.0085 - accuracy: 0.9973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f654b0e2310>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(\n",
    "    traintensor, # Training data\n",
    "    Y_train, # Labels of training data\n",
    "    batch_size=128, # Batch size for the optimizer algorithm\n",
    "    nb_epoch=20, # Number of epochs to run the optimizer algorithm\n",
    "    verbose=2 # Level of verbosity of the log messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 202us/step\n",
      "Test loss 0.029928781792297382\n",
      "Test accuracy 0.9926999807357788\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testtensor, Y_test)\n",
    "print(\"Test loss\", score[0])\n",
    "print(\"Test accuracy\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final exercise: LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=http://yann.lecun.com/exdb/lenet/>LeNet</a> is a particular convolutional neural network definition that has proven to be quite effective. As a final exercise we will build this network and try it on our digits problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Build and train a LeNet network, which is defined by the following layers:\n",
    "<ul>\n",
    "     <li>A Convolution layer of 20 channels, kernel size 5 and rectified linear activation</li>\n",
    "     <li>A MaxPooling layer of size 2 and stride 2 (check <a href=http://keras.io/layers/convolutional/>the docs</a>)</li>\n",
    "     <li>A 25% Dropout</li>\n",
    "     <li>A Convolution layer of 50 channels, kernel size 5 and rectified linear activation</li>\n",
    "     <li>A MaxPooling layer of size 2 and stride 2 (check <a href=http://keras.io/layers/convolutional/>the docs</a>)</li>\n",
    "     <li>A 25% Dropout</li>\n",
    "     <li>A Flatten layer</li>\n",
    "     <li>A Dense layer with 500 units and rectified linear activation</li>\n",
    "     <li>A 50% Dropout</li>\n",
    "     <li>An output Dense layer with softmax activation</li>\n",
    "</ul>\n",
    "Is this the best network so far for the problem?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/master/Aplicaciones/anaconda-navigator/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (5, 5), input_shape=(28, 28, 1..., padding=\"valid\")`\n",
      "  if __name__ == '__main__':\n",
      "/home/master/Aplicaciones/anaconda-navigator/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(50, (5, 5), input_shape=(28, 28, 1..., padding=\"valid\")`\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "kernel_size = 5 # Size of the kernel for the convolution layers\n",
    "pool_size = 2 # Size of the pooling region for the pooling layers\n",
    "stride=2\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(20, # Number convolution channels to generate\n",
    "                        kernel_size, kernel_size, # Size of convolution kernels\n",
    "                        border_mode='valid', # Strategy to deal with borders\n",
    "                        input_shape=(img_rows, img_cols, 1))) # Size = image rows x image columns x channels\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(pool_size, pool_size),strides=(stride,stride)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(50, # Number convolution channels to generate\n",
    "                        kernel_size, kernel_size, # Size of convolution kernels\n",
    "                        border_mode='valid', # Strategy to deal with borders\n",
    "                        input_shape=(img_rows, img_cols, 1))) # Size = image rows x image columns x channels\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.<br>\n",
    "                          THIS IS THE END OF THE ASSIGNMENT<br>\n",
    "~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.<br>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/pro.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Rebuild the LeNet network with a larger number of training epochs. What is the best test error you can achieve?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/pro.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "If your PC has a CUDA-compatible GPU card you can take advantage of it to significanly accelerate training times. You are encouraged to configure Keras to make use of your GPU.\n",
    " </td></tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
